---
layout:     post
title:      "Spark Inside(ALS)"
subtitle:   "(ALS推荐算法应用)Spark inside让普通开发人员可以通过该篇文档的学习可以快速上手Spark"
date:       2017-04-17 12:30:00
author:     "ryan"
header-img: "img/post-bg-01.jpg"
---

# Digest

该篇文章以极其简单的语言来描述ALS222推荐算法，以及如何基于Spark实现一个基于ALS推荐算法的分布式处理程序。



# 1. ALS算法推荐

互联网基本都是围绕人在产生信息，比如电商网用户购买产品，电影网用户看电影，音乐网用户听歌等等。但是由于对于歌，电影，商品，用户可能一辈子都买不完，看不完， 听不完这些东西。绝大部分用户，都只对其中的电影，歌，商品产生了关系，我们就要利用现有的这些关系来预测出没有和这些用户发生关系的哪些最有可能发生关系。这种关系在ALS算法里面，采用一个整数表示，比如豆瓣电影的用户对已经看过电影的评分；再比如网易云音乐对歌的收藏/切歌/循环听，这些用户行为动作产生的综合得分；再比如电商购买过，评价过，浏览过的产品，这些用户行为动作产证的综合得分。这些就会产生如下的一个稀疏矩阵：

|      | Item1 | Item2 | Item3 | Item4 |
| ---- | ----- | ----- | ----- | ----- |
| 用户1  | 5     |       | 1     |       |
| 用户2  |       | 2     |       |       |
| 用户3  |       |       | 3     | 4     |

ALS算法会将这个[用户xItem]的矩阵拆分为[用户xK]和[KxItem]两矩阵的乘积，这个K值就是ALS算法的核心，ALS算法要计算出这两个拆分后矩阵中的所有K值。

用户xK

|      | k1   | k2   | k3   | k4   | k5   |
| ---- | ---- | ---- | ---- | ---- | ---- |
| 用户1  | 2    | 1    | 2    | 1    | 1    |
| 用户2  | 1    | 0.5  | 3    | 0.3  | 2    |
| 用户3  | 4    | 3    | 1    | 4    | 2    |

KxItem

|      | Item1 | Item2 | Item3 | Item4 |
| ---- | ----- | ----- | ----- | ----- |
| k1   | 0.5   | 1     | 0.1   | 0.5   |
| k2   | 2     | 0.1   | 0.5   | 0.1   |
| k3   | 0.5   | 0.2   | 0.1   | 0.1   |
| k4   | 0.5   | 0.8   | 0.1   | 0.2   |
| k5   | 0.5   | 0.05  | 0.1   | 0.1   |

通过ALS算法拆分成这两个后，再两这两个拆分矩阵进行相乘，得到的结果和以前[用户xItem]已有的关系值，进行求误差值的方差求和，看看和远来已有用户和Item差多远。

ALS拆分后，再进行相乘得到的预测值：

|      | Item1           | Item2           | Item3 | Item3 |
| ---- | --------------- | --------------- | ----- | ----- |
| 用户1  | 5               | **<u>3.35</u>** | 1.1   | 1.6   |
| 用户2  | **<u>5.5</u>**  | 1.99            | 0.88  | 1.11  |
| 用户3  | **<u>11.5</u>** | 7.8             | 2.6   | 3.2   |

预测与实际

|           | 预测   | 实际   |
| --------- | ---- | ---- |
| 用户1-Item1 | 5    | 5    |
| 用户1-Item3 | 1.1  | 1    |
| 用户2-Item2 | 1.99 | 2    |
| 用户3-Item3 | 2.6  | 3    |
| 用户3-Item4 | 3.2  | 4    |

```java
误差值的方差求和＝|(5-5)^2+(1.1-1)^2+(1.99-2)^2+(2.6-3)^2+(3.2-4)^2|=|0.01-0.0001-0.16-0.64|=0.7901;
```

上面的拆分，预测与实际方差求和计算，会迭代多次(Spark ALS算法实现默认是10次)，最后选一个方差最小的作为预测结果；上面加粗和下划线就是用户1-3最应该推荐给他们的Item;

中间ALS的拆分具体算法实现，能力有限，这里就不解释了，但是我给一些链接，感兴趣可以自行去了解：

https://zhihu.com/question/37031188/answer/111336809







# 2. 基于Spark ALS函数实现推荐

## 2.1 Step by step Demo

1.初始化SparkContext，并且加载本地豆瓣用户与电影评分文件：

```java
SparkConf conf = new SparkConf().setAppName("appDouban");
JavaSparkContext sc = new JavaSparkContext(conf);
JavaRDD<String> rdd1 = sc.textFile("/Users/ryan/Downloads/data/user_movies_result.csv");
JavaRDD<Rating> rdd2 = rdd1.map(new Function<String,Rating>() {
@Override
public Rating call(String v1) throws Exception {
		String[] vals = v1.split(",");
		return new Rating(Integer.parseInt(vals[0]), Integer.parseInt(vals[1]), Double.parseDouble(vals[2]));
	}
});
```

2.初始化训练模型：

```java
//隐性因子的个数
int rank = 10;
//方差计算误差比较的迭代次数
int numIterations = 10;
//初始化训练模型
MatrixFactorizationModel model = ALS.train(JavaRDD.toRDD(rdd2), rank, numIterations, 0.01);
```

3.处理需要预测的user和product的关系集合：

```java
//找出user集合，并去重
JavaRDD<Integer> userRDD = rdd2.map(new Function<Rating, Integer>() {

	@Override
	public Integer call(Rating v1) throws Exception {
		// TODO Auto-generated method stub
		return v1.user();
	}
	
}).distinct();

//找出产品集合，并去重
JavaRDD<Integer> productRDD = rdd2.map(new Function<Rating, Integer>() {

	@Override
	public Integer call(Rating v1) throws Exception {
		// TODO Auto-generated method stub
		return v1.product();
	}
	
}).distinct();

//求待预测的user和product集合
JavaPairRDD<Integer, Integer> rdd3 = userRDD.cartesian(productRDD);
```

4.执行预测处理：

```java
JavaRDD<Rating> rdd4 = model.predict(rdd3);
```

5.去除之前已有用户和产品的预测关系值：

```java
//转换预测结果user-product和score的键值对
JavaPairRDD<String, Double> rdd5 = rdd4.mapToPair(new PairFunction<Rating, String, Double>() {
	@Override
	public Tuple2<String, Double> call(Rating t) throws Exception {
		return new Tuple2<String, Double>(String.format("%s-%s", t.user(), t.product()), t.rating());
	}
});

//转换之前user-product和score的键值对
JavaPairRDD<String,Double> rdd6 = rdd2.mapToPair(new PairFunction<Rating,String,Double>() {
	@Override
	public Tuple2<String, Double> call(Rating t) throws Exception {
		return new Tuple2<String,Double>(String.format("%s-%s", t.user(), t.product()), t.rating());
	}
});

//去除以前用户和产品已经主动关联的键值对
JavaPairRDD<String,Double> rdd7 = rdd5.subtractByKey(rdd6);
```

6.将预测结果保存并存储到MongDB，供下游应用调用：

```java
//将要推荐的user-product和socre的键值对，转换城mongodb支持的BSONObject对象
JavaPairRDD<Object, BSONObject> rdd8 = rdd7.mapToPair(new PairFunction<Tuple2<String,Double>,Object, BSONObject>(){

	@Override
	public Tuple2<Object, BSONObject> call(
			Tuple2<String, Double> t)
			throws Exception {
		ObjectId key = ObjectId.get();
		String[] keys = t._1.split("-");
		DBObject obj = (DBObject)JSON.parse(String.format("{\"userId\":%s,\"movieId\":%s,\"score\":%s}", keys[0], keys[1], t._2));
		obj.put("_id", key);
		return new Tuple2<Object,BSONObject>(key,obj);
	}
	
});

Configuration mongoConfig = new Configuration();
mongoConfig.set("mongo.auth.uri","mongodb://user:password@host/schema");	
mongoConfig.set("mongo.output.uri", "mongodb://host:ip/schema.collection_name");
//将预测出来的user和产品的推荐数据，存储到MongoDB里面去
rdd8.saveAsNewAPIHadoopFile("",Object.class, BSONObject.class, MongoOutputFormat.class, mongoConfig);
```

## 2.2 该算法有缺点：

### 2.2.1 优点：

需要推荐数据比较少，推荐准确性比较高。

### 2.2.2 缺点：

如果用户和产品比较多的话，此算法都需要确实数据的全量计算(计算量较大)，数据量多了以后，实施增量计算较为困难。